### 심층신경망
- 인공신경망에서 층을 더 늘리는 알고리즘
- 입력층과 출력층 사이에 밀집층(은닉층) 추가
- 은닉층에도 활성화 함수가 들어가 있음(활성화함수 : 신경만 층의 선형방정식의 계산값에 적용하는 함수)
- 출력층에는 활성화 함수가 제한되어 있음(이진분류:시그모이드, 다중분류:소프트맥스)
- 이유
  1. 은닉층에서 선형적인 산술 계산만 하면 수행역할이 없어지게 됨
  2. 선형계산을 적당히 비틀어야 다음 층의 계산과 단순히 합쳐지지 않고 나름의 역할을 수행
  3. 활성화 방정식을 적용시켜 선형을 적당히 비틀어 비선형화
- 대표 활성화 함수
  1. 시그모이드 함수 : 뉴런의 출력 z 값을 0과 1로 압축하는 역할
  2. 렐루 함수 : 임력이 양수일 경우 활성화 함수가 없는것처럼 그냥 통과 / 음수일경우에 0을 만들어버림
     1. 단점 : 층이 많은 신경망 일수록 누적되어 학습을 어렵게 했고 계선하기 위한 다른 종류의 활성화 함수가 제안됨

#### flatten 층
- 케라스는 직접 1차원으로 변경하도록 층을 하나 제공하는데 그 층이 Flatten 층임`(Flatten 클래스)`
- 입력층으로 추가

#### 옵티마이저 파라미터
- 하이퍼파라미터 배치나 조정을 가장 적절하게 바꿔주는 역할
- 학습의 마지막쯤에서 자동으로 배치사이즈나 학습율등을 조정하는 역할
- `Adam` : 모멘텀 최적화와 RMSprop의 장점을 접목한 옵티마이저 함수