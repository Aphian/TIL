### LSTM 알고리즘
- `Long Short Term Memory`
- 기존의 `RNN`이 출력과 먼 위치에 있는 정보를 기억할 수 없다는 단점을 보완
- 장/단기 기억을 가능하게 설계한 신경망의 구조
- 순서가 지날수록 은닉상태에 담긴 초기 정보는 희석됨
  - 물리적으로 앞의 모든 토근에 대한 가중치의 저장은 어려움
  - `LSTM`이 연구됨

#### LSTM 순환 상태 2가지
- 은닉상태 / 셀 상태
  - 셀 상태는 다음 층으로 전달 되지 않고 `LSTM` 셀에서 순환만 되는 값
- 이전 타임스탭 데이터에 새로 입력되는 타임스탭 입력값들을 누적시켜서 타임스탭이 지날때 마다 계속 새로운 값을 생성(긴 시퀸스를 기억하는 방법)
- **입력과 은닉상태에 곱해지는 가중치 w0 와 셸상태에 곱해지는 가중치 wf는 다른 값의 가중치여야함**