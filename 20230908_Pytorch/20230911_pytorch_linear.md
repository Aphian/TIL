### pytorch Linear Regression (선형 회귀)
- 훈련데이터 셋
  - 모델 학습에 데이터 `tensor(torch.tensor)`의 형태를 가지고 있어야함
  - 입력과 출력을 각기 다른 `tensor`에 저장할 필요가 있음.

#### 가설(Hypothesis) 수립
- 머신 러닝에서 가설은 임의로 추측해서 세워보는 식일수도 있고, 경험적으로 알고 있는 식
- 선형회귀에서의 가설 : 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일
$$y = Wx + b$$
#### 비용 함수(Cost function)
- 손실 함수 / 오차 함수 / 목점 함수와 같은 의미
- 오차의 제곱합에 대한 평균을 구할 수 있는데 이를 평균 제곱 오차`(Mean Squared Error, MSE)`
- `W 와 b`를 찾기 위한 최적화된 식
- 평균 제곱 오차의 값을 최소값으로 만드는 `W`와 `b`를 찾아내는 것이 가장 훈련 데이터를 잘 반영한 직선을 찾아내는 일

#### 옵티마이저 - 경사 하강법 (Grandient Descent)
- 비용 함수(Cost Function)의 값을 최소로 하는 `W(가중치)` 와  `b(편향)`를 찾는 방법 사용되는 것이 옵티마이저`(Optimizer)` 알고리즘

**가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념입니다. 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법**

#### 다중 선형 회귀
- $$H(x) = Wx + b$$ 
  - 단순 선형 회귀
- $$H(x) = W_{1}x_{1} + W_{2}x_{2} + W_{3}x_{3} + b$$
  - 다중 선형 회귀

#### nn.Module 로 선형 회귀
- `pytorch` 에서 선형 회귀 모델 `nn.Linear()` 함수
- 평균 제곱 오차 `nn.functional.mse_loss()` 함수

#### 미니 배치와 데이터 로드
- 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 개념 미니 배치
- 미니 배치 학습을 하게 되면 미니 배치 만큼만 가져가서 미니 배치에 대한 비용`(cost)`를 계산하고 경사 하강법을 수행
- 배치 경사 하강법은 경사 하강법을 할 때, 전체 데이터를 사용하므로 가중치 값이 최적값에 수렴하는 과정이 매우 안정적이지만, 계산량이 너무 많이 든다.
- 미니 배치 경사 하강법은 경사 하강법을 할 때, 전체 데이터의 일부만을 보고 수행하므로 최적값으로 수렴하는 과정에서 값이 조금 헤매기도 하지만 훈련 속도가 빠름.

#### 커스텀 데이터셋(Custom Dataset)
- `torch.utils.data.Dataset` `pytorch`에서 데이터셋을 제공하는 추상 클래스