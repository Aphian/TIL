### 순환 신경망 (Recurrent Neural Network)
- 시퀀스 모델
- 입력과 출력을 시퀀스 단위로 처리하는 모델
- `RNN` 딥러닝에서의 기본적인 시퀀스 모델
- 은닉층에서 활성화 함수를 지난 값은 출력층으로만 향함
  - -> 피드 포워드 신경망 `(Feed Forward Neural Network)`
- 은닉층의 노드에서 활성화 함수를 통해 나온 결과값으로 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고 있음
- 이전 값을 기억하려고 하는 일종의 메모리 역할 = 메모리 셀 / `RNN` 셀
- 은닉층의 메모리 셀은 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동

### 장단기 메모리 (Long Short-Term Memory, LSTM)
- 기본 `RNN`은 비교적 짧은 시퀀스에 대해서만 효과를 보이는 단점 (장기 의존성 문제)로 `LSTM` 구현
- 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함
- 은닉 상태를 계산하는 식이 기본 `RNN`보다 조금 더 복잡해지고 **셀 상태**라는값을 추가
- 각각의 게이트는 시그모이드 함수가 존재함

### 게이트 순환 유닛 (Gated Recurrent Unit, GPU)
- `LSTM`에서의 장기 의존성 문제 해결책을 유지하면서 은식 상태를 업데이트하는 계산을 줄임
- 즉, `GPU`는 성능은 `LSTM`과 유사하면서 구조를 간단화
- 업데이트 게이트 / 리셋 게이트 두 가지 게이트만 존재