### 자연어 처리 원-핫 인코딩
- 자연어 처리에서는 문자를 숫자로 바꾸는 기법이 있음
 - 원-핫 인코딩 가장 기본적인 방법

#### 원-핫 인코딩
- 단어 집합 : **서로 다른 단어**들의 집합
  - 텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 집합
- 집합에 고유한 숫자를 부여하는 정수 인코딩
- 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 `1`의 값을 부여하고, 다른 인덱스에는 `0`을 부여하는 단어의 벡터 표현 방식
- 이렇게 표현된 벡터를 `원-핫 벡터`라고 함
1. 각 단어에 고유한 인덱스 부여 (정수 인코딩)
2. 표현하고 싶은 단어의 인덱스 `1` / 다른 단어의 인덱스 `0`

#### 단점
- 표현 방식의 단어의 개수가 늘어날수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점
- 벡터의 차원이 계속 늘어난다
- 유사도를 표현하지 못함.
- 희소 표현
  - 벡터 또는 행렬의 값이 대부분이 `0`으로 표현되는 방법
  - `희소 벡터`라고도 함
  - 단어 표현 방식에 따라 차원이 계속 늘어남
  - 유사도 표현을 할 수 없음

### 워드 임베딩 (Word Embedding)
- 단어를 벡터로 표현하는 것을 말한다.
- 단어를 밀집 표현으로 변환하는 방법
- 밀집 표현
  - 벡터의 차원을 단어 집합의 크기로 상정하지 않음
  - 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
- `임베딩 벡터`라고도 함

|-|원-핫 벡터|임베딩 벡터
|---|---|---|
|차원|고차원(단어의 집합 크기)|저차원|
|다른 표현|희소 벡터의 일종|밀집 벡터의 일종|
|표현 방법|수동|훈련 데이터로부터 학습함|
|값의 타입|1과0|실수|

### 워드투 벡터(Word2Vec)
- 원-핫 인코딩에서의 유사도 문제를 보완하기 위한 개념
- 분산 표현 : `비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다` 라는 가정
- 저차원에 단어의 의미를 여러차원에다가 분산하여 표현
- 단어 간 유사도 계산 가능
- `NNLM` , `RNNLM` 등이 있으나 `Word2Vec` 많이 사용
- `CBOW (Continuous Bag of Words)` / `Skip-Gram`
- `CBOW`
  - 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측
- `Skip-Gram`
  - 중간에 있는 단어로 주변을 예측하는 방법
- 네거티브 샘플링 `(Negative Sampling)`
- `SGNS (Skip-Gram with Negative Sampling)`
  - `Word2Vec`에서는 속도가 문제점으로 지목됨
  - 조금 더 효율적으로 단어 임베딩 조정을 위한 방법
  - 전체 단어 집합 보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 바꾸는 작업
